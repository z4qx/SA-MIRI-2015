######################################################################################################
#### THIS IS A TEMPLATE FILE FOR spark4mn 3.2.0. DO NOT USE THIS TEMPLATE WITH ANY OTHER VERSION. ####
######################################################################################################

### JOB PARAMETERS: ###
  ## OPTIONAL: ##
    # JOB_NAME="[STRING]"        # Default: Resource file name. Name of the job as seen via cluster's resource manager.
    # WORKING_DIR="[PATH]"       # Default: Current working directory. User must have R/W/X. Main working directory for the job.
    # TMP_DIR="[PATH]"           # Default: Ignored: TMPDIR is used. User must have R/W/X. Main temporal directory for the job.
    # WALLCLOCK=[INT] # Minutes. # Default: 15. Minimum: 1. Maximum allowed execution time.
    # NEAR_NODES="[BOOL]"        # Default: false. Should the nodes be connected to the minimum number of switches?
    # RESERVATION="[STRING]"     # Default: Ignored: no reservation. Specific machine reservation.
    # QUEUE="[STRING]"           # Default: Ignored: default queue. Specific queue.
    # X11="[BOOL]"               # Default: false. Needed if the job uses graphics, otherwise the job will fail.


### SPARK_PARAMETERS: ###
  ## OPTIONAL: ##
    # SPARK_VERSION="[STRING]"                              # Default: Default module vesion. Must match with a proper Spark module version.
    # SPARK_PACKAGES="[[NAME | NAME/VERSION], ...]"         # Default: Ignored. If additional Spark packages are needed. See /apps/SPARK/SPARK_VERSION/package/package.list
    # SPARK_NNODES=[INT]                                    # Default: 1. Minimum: 1. It will be 1 more node to host master services.
    # SPARK_NWORKERS_PER_NODE=[INT]                         # Default: It depends on resource distribution. Minimum: 1.
    # SPARK_NCORES_PER_WORKER=[INT]                         # Default: It depends on resource distribution. Minimum: 1.
    # SPARK_WORKER_MEM_SIZE=[INT] # Megabytes.              # Default: It depends on resource distribution. Minimum: 1.
    # SPARK_WORKER_AFFINITY="[CORE | SOCKET | NODE | NONE]" # Default: NONE.
    # SPARK_NETWORK_INTERFACE="[IB | ETH]"                  # Default: IB. 40 Gb/s Infiniband, and 1 Gb/s full duplex Ethernet.


### DFS PARAMETERS: ###
  ## OPTIONAL: ##
    # DFS_MODULE="[HDFS | CASSANDRA | NONE]" # Default: NONE.
  
  ## OPTIONAL, BUT DISABLED IF DFS_MODULE="NONE": ##
    # DFS_SHARE_NODES="[BOOL]"                            # Default: false. Will DFS share nodes with Spark?
    # DFS_NNODES=[INT]                                    # Default: SPARK_NNODES. Minimum: 1. It will be 1 more node to host master services.
    # DFS_NWORKERS_PER_NODE=[INT]                         # Default: It depends on resource distribution. Minimum: 1.
    # DFS_NCORES_PER_WORKER=[INT]                         # Default: It depends on resource distribution. Minimum: 1.
    # DFS_WORKER_MEM_SIZE=[INT] # Megabytes.              # Default: It depends on resource distribution. Minimum: 1.
    # DFS_WORKER_AFFINITY="[CORE | SOCKET | NODE | NONE]" # Default: NONE.
    # DFS_NETWORK_INTERFACE="[IB | ETH]"                  # Default: IB. 40 Gb/s Infiniband, and 1 Gb/s full duplex Ethernet.


### PROBLEM PARAMETERS: ###
  ## COMPULSORY: ##
    DOMAIN_JAR_1="/home/samXX/samXXXXX/spark.helloworld-1.0.0.jar"           # JAR file for each main problem to be executed.
    DOMAIN_ENTRY_POINT_1="bsc.Main" # Entry class for each main problem to be executed.

  
  ## OPTIONAL: ##
    # ADDITIONAL_MODULES="[[NAME | NAME/VERSION], ...]" # Default: Ignored. If additional modules are needed.
    # ADDITIONAL_CLASSPATH="[PATH:PATH:...]"            # Default: Ignored. If additional classes are needed. Has precedence over ADDITIONAL_MODULES.
    # ADDITIONAL_LIBRARIES="[PATH:PATH:...]"            # Default: Ignored. If additional native libraries are needed. Has precedence over ADDITIONAL_MODULES.
    # PROLOG_JAR_[[1-9]+]="[PATH]"                      # Default: Ignored. JAR file for each program to be executed before the main problems.
    # EPILOG_JAR_[[1-9]+]="[PATH]"                      # Default: Ignored. JAR file for each program to be executed after the main problems.
    # PROLOG_ENTRY_POINT_[[1-9]+]="[STRING]"            # Default: Ignored. Entry class for each program to be executed before the main problems.
    # EPILOG_ENTRY_POINT_[[1-9]+]="[STRING]"            # Default: Ignored. Entry class for each program to be executed after the main problems.
    
    ## Al parameter paths must be declared as [[IN | OUT]_[F | D] PATH]. If you use a non-tagged path it will not be checked.
      # PROLOG_PARAMETERS_[[1-9]+]="[STRING, ...]"        # Default: Ignored. Parameters for each prologue.
      DOMAIN_PARAMETERS_1="10000, 100, 100, 10, 10, 3, 100"        # Default: Ignored. Parameters for each domain.

      # EPILOG_PARAMETERS_[[1-9]+]="[STRING, ...]"        # Default: Ignored. Parameters for each epilogue.
